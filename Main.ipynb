{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) \n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', message = \"Pandas requires version .* or newer of 'bottleneck'.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting penalty=None will ignore the C\")\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report, recall_score, precision_recall_curve, average_precision_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".dat\"):\n",
    "        record = wfdb.rdrecord(file[:-4], sampfrom=0, sampto=None)\n",
    "        signals = record.p_signal\n",
    "        raw_signals = record.d_signal\n",
    "        signal_names = record.sig_name\n",
    "        df = pd.DataFrame(data = signals, columns=record.sig_name)\n",
    "        df['time'] = np.arange(len(df)) / record.fs\n",
    "        # print(df.head())\n",
    "        df.to_csv(file[:-4] + \".csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject9_SpO2HR.dat',\n",
       " 'Subject11_SpO2HR.dat',\n",
       " 'Subject12_SpO2HR.hea',\n",
       " 'Subject19_SpO2HR_sync_merged.csv',\n",
       " 'Subject17_SpO2HR_sync.csv',\n",
       " 'Subject3_merged_fused.csv',\n",
       " 'Subject13_SpO2HR_sync_merged.csv',\n",
       " 'Subject19_merged_fused.csv',\n",
       " 'Subject2_merged.csv',\n",
       " 'Subject18_AccTempEDA.dat',\n",
       " 'Subject17_SpO2HR_merged.csv',\n",
       " 'Subject4_SpO2HR_sync.csv',\n",
       " 'Subject10_AccTempEDA.dat',\n",
       " 'Subject7_SpO2HR.csv',\n",
       " 'Subject2_AccTempEDA.dat',\n",
       " 'Subject12_merged_fused.csv',\n",
       " 'Subject17_merged.csv',\n",
       " 'Subject20_merged.csv',\n",
       " 'Subject14_SpO2HR_sync_merged.csv',\n",
       " 'Subject12_SpO2HR_merged.csv',\n",
       " 'Subject7_SpO2HR.hea',\n",
       " 'Subject4_SpO2HR.dat',\n",
       " 'Subject8_merged_fused.csv',\n",
       " 'Main.ipynb',\n",
       " 'Subject18_SpO2HR_sync.csv',\n",
       " 'Subject17_AccTempEDA.dat',\n",
       " 'Subject12_SpO2HR.csv',\n",
       " 'RECORDS',\n",
       " 'Subject5_AccTempEDA.dat',\n",
       " 'Subject16_AccTempEDA.dat',\n",
       " 'Subject19_SpO2HR.dat',\n",
       " 'Subject17_SpO2HR_sync_merged.csv',\n",
       " 'Subject1_SpO2HR.dat',\n",
       " 'Subject2_SpO2HR.hea',\n",
       " 'Subject4_AccTempEDA.dat',\n",
       " 'Subject12_merged.csv',\n",
       " 'Subject16_SpO2HR_merged.csv',\n",
       " 'Subject17_SpO2HR.csv',\n",
       " 'Subject20_SpO2HR.csv',\n",
       " 'Subject16_merged_fused.csv',\n",
       " 'Subject11_AccTempEDA.dat',\n",
       " 'Subject10_SpO2HR_sync_merged.csv',\n",
       " 'Subject7_merged.csv',\n",
       " 'Subject13_SpO2HR_merged.csv',\n",
       " 'Subject20_AccTempEDA.atr',\n",
       " 'Subject17_SpO2HR.hea',\n",
       " 'Subject3_SpO2HR_sync.csv',\n",
       " 'Subject14_SpO2HR.dat',\n",
       " 'Subject20_SpO2HR.hea',\n",
       " 'Subject3_AccTempEDA.dat',\n",
       " 'Subject7_merged_fused.csv',\n",
       " 'subjectinfo.csv',\n",
       " 'Subject2_SpO2HR.csv',\n",
       " 'Subject10_SpO2HR_sync.csv',\n",
       " 'Subject19_AccTempEDA.dat',\n",
       " 'Subject1_AccTempEDA.dat',\n",
       " 'Subject5_SpO2HR.csv',\n",
       " 'Subject20_SpO2HR_sync_merged.csv',\n",
       " 'Subject8_SpO2HR_merged.csv',\n",
       " 'Subject13_AccTempEDA.dat',\n",
       " 'Subject15_SpO2HR_merged.csv',\n",
       " 'Subject20_AccTempEDA.hea',\n",
       " 'Subject8_SpO2HR.hea',\n",
       " 'Subject10_SpO2HR.hea',\n",
       " 'Subject13_SpO2HR.dat',\n",
       " 'Subject18_merged_fused.csv',\n",
       " 'Subject18_merged.csv',\n",
       " 'Subject2_merged_fused.csv',\n",
       " 'Subject9_AccTempEDA.dat',\n",
       " 'Subject11_SpO2HR_sync_merged.csv',\n",
       " 'ANNOTATORS',\n",
       " 'Subject6_AccTempEDA.dat',\n",
       " 'Subject2_SpO2HR_sync.csv',\n",
       " 'Subject10_SpO2HR_merged.csv',\n",
       " 'Subject10_SpO2HR.csv',\n",
       " 'Subject14_AccTempEDA.dat',\n",
       " 'Subject20_AccTempEDA.csv',\n",
       " 'Subject8_SpO2HR.csv',\n",
       " 'Subject11_SpO2HR_sync.csv',\n",
       " 'Subject3_merged_fused_engineered.csv',\n",
       " 'Subject9_merged_fused.csv',\n",
       " 'Subject15_merged.csv',\n",
       " 'Subject6_SpO2HR.dat',\n",
       " 'Subject5_SpO2HR.hea',\n",
       " 'Subject13_merged_fused.csv',\n",
       " 'Subject16_SpO2HR_sync_merged.csv',\n",
       " 'Subject15_SpO2HR.csv',\n",
       " 'Subject9_SpO2HR_merged.csv',\n",
       " 'Subject17_merged_fused.csv',\n",
       " 'Subject14_SpO2HR_merged.csv',\n",
       " 'Subject16_SpO2HR_sync.csv',\n",
       " 'Subject7_AccTempEDA.dat',\n",
       " 'Subject15_SpO2HR_sync_merged.csv',\n",
       " 'Subject18_SpO2HR.hea',\n",
       " 'Subject3_SpO2HR.dat',\n",
       " 'Subject5_SpO2HR_sync.csv',\n",
       " 'Subject8_merged.csv',\n",
       " 'Subject15_AccTempEDA.dat',\n",
       " 'Subject20_SpO2HR_merged.csv',\n",
       " 'Subject10_merged.csv',\n",
       " 'Subject8_AccTempEDA.dat',\n",
       " 'Subject11_SpO2HR_merged.csv',\n",
       " 'Subject6_merged_fused.csv',\n",
       " 'Subject18_SpO2HR.csv',\n",
       " 'Subject12_SpO2HR_sync_merged.csv',\n",
       " 'Subject5_merged.csv',\n",
       " 'Subject19_SpO2HR_sync.csv',\n",
       " 'Subject12_AccTempEDA.dat',\n",
       " 'Subject16_SpO2HR.dat',\n",
       " 'Subject15_SpO2HR.hea',\n",
       " 'Subject18_SpO2HR_sync_merged.csv',\n",
       " 'Subject15_AccTempEDA.hea',\n",
       " 'Subject18_SpO2HR_merged.csv',\n",
       " 'Subject13_merged.csv',\n",
       " 'Subject7_AccTempEDA.hea',\n",
       " 'Subject5_SpO2HR_merged.csv',\n",
       " 'Subject18_SpO2HR.dat',\n",
       " 'Subject3_SpO2HR.hea',\n",
       " 'Subject8_AccTempEDA.csv',\n",
       " 'Subject9_SpO2HR_sync_merged.csv',\n",
       " 'Subject17_AccTempEDA.atr',\n",
       " 'Subject16_SpO2HR.csv',\n",
       " 'Subject12_AccTempEDA.csv',\n",
       " 'Subject10_merged_fused.csv',\n",
       " 'Subject5_AccTempEDA.atr',\n",
       " 'Subject3_SpO2HR_sync_merged.csv',\n",
       " 'Subject13_SpO2HR_sync.csv',\n",
       " 'Subject12_AccTempEDA.hea',\n",
       " 'Subject15_SpO2HR.dat',\n",
       " 'Subject16_SpO2HR.hea',\n",
       " 'Subject18_AccTempEDA.atr',\n",
       " 'Subject6_merged.csv',\n",
       " 'Subject8_AccTempEDA.hea',\n",
       " 'Subject3_SpO2HR.csv',\n",
       " 'Subject10_AccTempEDA.atr',\n",
       " 'Subject7_AccTempEDA.csv',\n",
       " 'Subject2_AccTempEDA.atr',\n",
       " 'Subject4_SpO2HR_sync_merged.csv',\n",
       " 'Subject1_merged_fused.csv',\n",
       " 'Subject15_AccTempEDA.csv',\n",
       " 'Subject6_AccTempEDA.csv',\n",
       " 'Subject19_SpO2HR_merged.csv',\n",
       " 'Subject3_merged.csv',\n",
       " 'Subject11_AccTempEDA.atr',\n",
       " 'Subject9_AccTempEDA.hea',\n",
       " 'Subject4_SpO2HR_merged.csv',\n",
       " 'Subject8_SpO2HR.dat',\n",
       " 'Subject20_AccTempEDA.dat',\n",
       " 'Subject7_SpO2HR_sync.csv',\n",
       " 'Subject5_merged_fused.csv',\n",
       " 'Subject14_AccTempEDA.csv',\n",
       " 'Subject3_AccTempEDA.atr',\n",
       " 'Subject13_SpO2HR.hea',\n",
       " 'Subject10_SpO2HR.dat',\n",
       " 'Subject7_SpO2HR_sync_merged.csv',\n",
       " 'Subject13_AccTempEDA.hea',\n",
       " 'Subject1_AccTempEDA.hea',\n",
       " 'Subject19_AccTempEDA.atr',\n",
       " 'Subject6_SpO2HR.csv',\n",
       " 'Subject14_SpO2HR_sync.csv',\n",
       " 'Subject16_AccTempEDA.atr',\n",
       " 'Subject5_SpO2HR.dat',\n",
       " 'Subject6_SpO2HR.hea',\n",
       " 'Subject1_AccTempEDA.csv',\n",
       " 'Subject4_AccTempEDA.atr',\n",
       " 'Subject13_AccTempEDA.csv',\n",
       " 'Subject16_merged.csv',\n",
       " 'Subject14_merged_fused.csv',\n",
       " 'Subject1_SpO2HR_merged.csv',\n",
       " 'Subject13_SpO2HR.csv',\n",
       " 'Subject14_AccTempEDA.hea',\n",
       " 'Subject9_AccTempEDA.csv',\n",
       " 'Subject8_SpO2HR_sync.csv',\n",
       " 'Subject6_AccTempEDA.hea',\n",
       " 'Subject11_AccTempEDA.csv',\n",
       " 'Subject6_SpO2HR_sync.csv',\n",
       " 'Subject6_AccTempEDA.atr',\n",
       " 'Subject11_merged_fused.csv',\n",
       " 'Subject3_AccTempEDA.csv',\n",
       " 'Subject14_SpO2HR.csv',\n",
       " 'Subject7_SpO2HR_merged.csv',\n",
       " 'Subject1_SpO2HR_sync_merged.csv',\n",
       " 'Subject14_AccTempEDA.atr',\n",
       " 'Subject4_AccTempEDA.hea',\n",
       " 'Subject9_merged.csv',\n",
       " 'Subject15_SpO2HR_sync.csv',\n",
       " 'Subject11_merged.csv',\n",
       " 'Subject19_SpO2HR.hea',\n",
       " 'Subject16_AccTempEDA.hea',\n",
       " 'Subject19_AccTempEDA.csv',\n",
       " 'Subject2_SpO2HR.dat',\n",
       " 'Subject1_SpO2HR.hea',\n",
       " 'Subject1_SpO2HR.csv',\n",
       " 'Subject1_AccTempEDA.atr',\n",
       " 'Subject19_AccTempEDA.hea',\n",
       " 'Subject19_SpO2HR.csv',\n",
       " 'Subject16_AccTempEDA.csv',\n",
       " 'Subject13_AccTempEDA.atr',\n",
       " 'Subject6_SpO2HR_sync_merged.csv',\n",
       " 'Subject4_AccTempEDA.csv',\n",
       " 'Subject2_SpO2HR_merged.csv',\n",
       " 'Subject9_SpO2HR_sync.csv',\n",
       " 'Subject20_SpO2HR.dat',\n",
       " 'Subject3_AccTempEDA.hea',\n",
       " 'Subject14_SpO2HR.hea',\n",
       " 'Subject17_SpO2HR.dat',\n",
       " 'Subject11_AccTempEDA.hea',\n",
       " 'Subject9_AccTempEDA.atr',\n",
       " 'Subject4_merged.csv',\n",
       " 'Subject2_AccTempEDA.hea',\n",
       " 'Subject5_SpO2HR_sync_merged.csv',\n",
       " 'SHA256SUMS.txt',\n",
       " 'Subject4_SpO2HR.csv',\n",
       " 'Subject8_AccTempEDA.atr',\n",
       " 'Subject10_AccTempEDA.hea',\n",
       " 'Subject6_SpO2HR_merged.csv',\n",
       " 'Subject19_merged.csv',\n",
       " 'Subject17_AccTempEDA.csv',\n",
       " 'Subject18_AccTempEDA.hea',\n",
       " 'Subject1_merged.csv',\n",
       " 'Subject4_merged_fused.csv',\n",
       " 'Subject9_SpO2HR.hea',\n",
       " 'Subject5_AccTempEDA.csv',\n",
       " 'Subject20_merged_fused.csv',\n",
       " 'Subject12_SpO2HR.dat',\n",
       " 'Subject11_SpO2HR.hea',\n",
       " 'Subject20_SpO2HR_sync.csv',\n",
       " 'Subject12_AccTempEDA.atr',\n",
       " 'Subject11_SpO2HR.csv',\n",
       " 'Subject9_SpO2HR.csv',\n",
       " 'Subject5_AccTempEDA.hea',\n",
       " 'Subject2_SpO2HR_sync_merged.csv',\n",
       " 'Subject18_AccTempEDA.csv',\n",
       " 'Subject12_SpO2HR_sync.csv',\n",
       " 'Subject8_SpO2HR_sync_merged.csv',\n",
       " 'Subject17_AccTempEDA.hea',\n",
       " 'Subject3_SpO2HR_merged.csv',\n",
       " 'Subject15_merged_fused.csv',\n",
       " 'Subject7_AccTempEDA.atr',\n",
       " 'Subject4_SpO2HR.hea',\n",
       " 'Subject7_SpO2HR.dat',\n",
       " 'Subject10_AccTempEDA.csv',\n",
       " 'Subject15_AccTempEDA.atr',\n",
       " 'Subject1_SpO2HR_sync.csv',\n",
       " 'Subject14_merged.csv',\n",
       " 'Subject2_AccTempEDA.csv']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    if file.endswith('_SpO2HR.csv'):\n",
    "        df = pd.read_csv(file)\n",
    "        # print(df.head())\n",
    "        df.drop(columns = ['time'], inplace=True)\n",
    "        sync_df=pd.DataFrame(np.repeat(df.values, 8, axis=0), columns=df.columns)\n",
    "        sync_df['time'] = np.arange(len(sync_df)) / 8\n",
    "        sync_df.to_csv(file[:-4] + \"_sync.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    # for each subject, read in the sync file and the acc file\n",
    "\n",
    "    for i in range(1,21):\n",
    "        if file.startswith(\"Subject\" + str(i)) and file.endswith(\"_sync.csv\"):\n",
    "\n",
    "            # read file in\n",
    "            sync_df = pd.read_csv(file)\n",
    "\n",
    "            # add other dataframe to sync df\n",
    "            for other_file in os.listdir():\n",
    "                if other_file.startswith(\"Subject\" + str(i)) and other_file.endswith(\"AccTempEDA.csv\"):\n",
    "                    acc_df = pd.read_csv(other_file)\n",
    "\n",
    "                    # merge dataframes\n",
    "                    merged_df = pd.merge(sync_df, acc_df, how='inner', on='time')\n",
    "\n",
    "                    # add physiological stress column\n",
    "                    ## need the seconds for the physiological stress column -- each entry is 1/8 of a second, so need entries between 5 minutes and 10 minutes -- 5*60*8 for the start of the 5 minutes, then need 5 minutes after that so 10*60*8 is when the 5 minutes ends\n",
    "                    merged_df['physiological_stress'] = np.where((merged_df['time'] >= 5*60*8) & (merged_df['time'] <= 10*60*8), 1, 0)\n",
    "                    \n",
    "                    # save merged dataframe\n",
    "                    merged_df.to_csv(\"Subject\" + str(i) + \"_merged.csv\", index=False)\n",
    "\n",
    "# now we have all the merged dataframes, need to split them into training, validation, and testing sets -- each subject is named subjectX_merged.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject9_SpO2HR.dat',\n",
       " 'Subject11_SpO2HR.dat',\n",
       " 'Subject12_SpO2HR.hea',\n",
       " 'Subject19_SpO2HR_sync_merged.csv',\n",
       " 'Subject17_SpO2HR_sync.csv',\n",
       " 'Subject3_merged_fused.csv',\n",
       " 'Subject13_SpO2HR_sync_merged.csv',\n",
       " 'Subject19_merged_fused.csv',\n",
       " 'Subject2_merged.csv',\n",
       " 'Subject18_AccTempEDA.dat',\n",
       " 'Subject17_SpO2HR_merged.csv',\n",
       " 'Subject4_SpO2HR_sync.csv',\n",
       " 'Subject10_AccTempEDA.dat',\n",
       " 'Subject7_SpO2HR.csv',\n",
       " 'Subject2_AccTempEDA.dat',\n",
       " 'Subject12_merged_fused.csv',\n",
       " 'Subject17_merged.csv',\n",
       " 'Subject20_merged.csv',\n",
       " 'Subject14_SpO2HR_sync_merged.csv',\n",
       " 'Subject12_SpO2HR_merged.csv',\n",
       " 'Subject7_SpO2HR.hea',\n",
       " 'Subject4_SpO2HR.dat',\n",
       " 'Subject8_merged_fused.csv',\n",
       " 'Main.ipynb',\n",
       " 'Subject18_SpO2HR_sync.csv',\n",
       " 'Subject17_AccTempEDA.dat',\n",
       " 'Subject12_SpO2HR.csv',\n",
       " 'RECORDS',\n",
       " 'Subject5_AccTempEDA.dat',\n",
       " 'Subject16_AccTempEDA.dat',\n",
       " 'Subject19_SpO2HR.dat',\n",
       " 'Subject17_SpO2HR_sync_merged.csv',\n",
       " 'Subject1_SpO2HR.dat',\n",
       " 'Subject2_SpO2HR.hea',\n",
       " 'Subject4_AccTempEDA.dat',\n",
       " 'Subject12_merged.csv',\n",
       " 'Subject16_SpO2HR_merged.csv',\n",
       " 'Subject17_SpO2HR.csv',\n",
       " 'Subject20_SpO2HR.csv',\n",
       " 'Subject16_merged_fused.csv',\n",
       " 'Subject11_AccTempEDA.dat',\n",
       " 'Subject10_SpO2HR_sync_merged.csv',\n",
       " 'Subject7_merged.csv',\n",
       " 'Subject13_SpO2HR_merged.csv',\n",
       " 'Subject20_AccTempEDA.atr',\n",
       " 'Subject17_SpO2HR.hea',\n",
       " 'Subject3_SpO2HR_sync.csv',\n",
       " 'Subject14_SpO2HR.dat',\n",
       " 'Subject20_SpO2HR.hea',\n",
       " 'Subject3_AccTempEDA.dat',\n",
       " 'Subject7_merged_fused.csv',\n",
       " 'subjectinfo.csv',\n",
       " 'Subject2_SpO2HR.csv',\n",
       " 'Subject10_SpO2HR_sync.csv',\n",
       " 'Subject19_AccTempEDA.dat',\n",
       " 'Subject1_AccTempEDA.dat',\n",
       " 'Subject5_SpO2HR.csv',\n",
       " 'Subject20_SpO2HR_sync_merged.csv',\n",
       " 'Subject8_SpO2HR_merged.csv',\n",
       " 'Subject13_AccTempEDA.dat',\n",
       " 'Subject15_SpO2HR_merged.csv',\n",
       " 'Subject20_AccTempEDA.hea',\n",
       " 'Subject8_SpO2HR.hea',\n",
       " 'Subject10_SpO2HR.hea',\n",
       " 'Subject13_SpO2HR.dat',\n",
       " 'Subject18_merged_fused.csv',\n",
       " 'Subject18_merged.csv',\n",
       " 'Subject2_merged_fused.csv',\n",
       " 'Subject9_AccTempEDA.dat',\n",
       " 'Subject11_SpO2HR_sync_merged.csv',\n",
       " 'ANNOTATORS',\n",
       " 'Subject6_AccTempEDA.dat',\n",
       " 'Subject2_SpO2HR_sync.csv',\n",
       " 'Subject10_SpO2HR_merged.csv',\n",
       " 'Subject10_SpO2HR.csv',\n",
       " 'Subject14_AccTempEDA.dat',\n",
       " 'Subject20_AccTempEDA.csv',\n",
       " 'Subject8_SpO2HR.csv',\n",
       " 'Subject11_SpO2HR_sync.csv',\n",
       " 'Subject3_merged_fused_engineered.csv',\n",
       " 'Subject9_merged_fused.csv',\n",
       " 'Subject15_merged.csv',\n",
       " 'Subject6_SpO2HR.dat',\n",
       " 'Subject5_SpO2HR.hea',\n",
       " 'Subject13_merged_fused.csv',\n",
       " 'Subject16_SpO2HR_sync_merged.csv',\n",
       " 'Subject15_SpO2HR.csv',\n",
       " 'Subject9_SpO2HR_merged.csv',\n",
       " 'Subject17_merged_fused.csv',\n",
       " 'Subject14_SpO2HR_merged.csv',\n",
       " 'Subject16_SpO2HR_sync.csv',\n",
       " 'Subject7_AccTempEDA.dat',\n",
       " 'Subject15_SpO2HR_sync_merged.csv',\n",
       " 'Subject18_SpO2HR.hea',\n",
       " 'Subject3_SpO2HR.dat',\n",
       " 'Subject5_SpO2HR_sync.csv',\n",
       " 'Subject8_merged.csv',\n",
       " 'Subject15_AccTempEDA.dat',\n",
       " 'Subject20_SpO2HR_merged.csv',\n",
       " 'Subject10_merged.csv',\n",
       " 'Subject8_AccTempEDA.dat',\n",
       " 'Subject11_SpO2HR_merged.csv',\n",
       " 'Subject6_merged_fused.csv',\n",
       " 'Subject18_SpO2HR.csv',\n",
       " 'Subject12_SpO2HR_sync_merged.csv',\n",
       " 'Subject5_merged.csv',\n",
       " 'Subject19_SpO2HR_sync.csv',\n",
       " 'Subject12_AccTempEDA.dat',\n",
       " 'Subject16_SpO2HR.dat',\n",
       " 'Subject15_SpO2HR.hea',\n",
       " 'Subject18_SpO2HR_sync_merged.csv',\n",
       " 'Subject15_AccTempEDA.hea',\n",
       " 'Subject18_SpO2HR_merged.csv',\n",
       " 'Subject13_merged.csv',\n",
       " 'Subject7_AccTempEDA.hea',\n",
       " 'Subject5_SpO2HR_merged.csv',\n",
       " 'Subject18_SpO2HR.dat',\n",
       " 'Subject3_SpO2HR.hea',\n",
       " 'Subject8_AccTempEDA.csv',\n",
       " 'Subject9_SpO2HR_sync_merged.csv',\n",
       " 'Subject17_AccTempEDA.atr',\n",
       " 'Subject16_SpO2HR.csv',\n",
       " 'Subject12_AccTempEDA.csv',\n",
       " 'Subject10_merged_fused.csv',\n",
       " 'Subject5_AccTempEDA.atr',\n",
       " 'Subject3_SpO2HR_sync_merged.csv',\n",
       " 'Subject13_SpO2HR_sync.csv',\n",
       " 'Subject12_AccTempEDA.hea',\n",
       " 'Subject15_SpO2HR.dat',\n",
       " 'Subject16_SpO2HR.hea',\n",
       " 'Subject18_AccTempEDA.atr',\n",
       " 'Subject6_merged.csv',\n",
       " 'Subject8_AccTempEDA.hea',\n",
       " 'Subject3_SpO2HR.csv',\n",
       " 'Subject10_AccTempEDA.atr',\n",
       " 'Subject7_AccTempEDA.csv',\n",
       " 'Subject2_AccTempEDA.atr',\n",
       " 'Subject4_SpO2HR_sync_merged.csv',\n",
       " 'Subject1_merged_fused.csv',\n",
       " 'Subject15_AccTempEDA.csv',\n",
       " 'Subject6_AccTempEDA.csv',\n",
       " 'Subject19_SpO2HR_merged.csv',\n",
       " 'Subject3_merged.csv',\n",
       " 'Subject11_AccTempEDA.atr',\n",
       " 'Subject9_AccTempEDA.hea',\n",
       " 'Subject4_SpO2HR_merged.csv',\n",
       " 'Subject8_SpO2HR.dat',\n",
       " 'Subject20_AccTempEDA.dat',\n",
       " 'Subject7_SpO2HR_sync.csv',\n",
       " 'Subject5_merged_fused.csv',\n",
       " 'Subject14_AccTempEDA.csv',\n",
       " 'Subject3_AccTempEDA.atr',\n",
       " 'Subject13_SpO2HR.hea',\n",
       " 'Subject10_SpO2HR.dat',\n",
       " 'Subject7_SpO2HR_sync_merged.csv',\n",
       " 'Subject13_AccTempEDA.hea',\n",
       " 'Subject1_AccTempEDA.hea',\n",
       " 'Subject19_AccTempEDA.atr',\n",
       " 'Subject6_SpO2HR.csv',\n",
       " 'Subject14_SpO2HR_sync.csv',\n",
       " 'Subject16_AccTempEDA.atr',\n",
       " 'Subject5_SpO2HR.dat',\n",
       " 'Subject6_SpO2HR.hea',\n",
       " 'Subject1_AccTempEDA.csv',\n",
       " 'Subject4_AccTempEDA.atr',\n",
       " 'Subject13_AccTempEDA.csv',\n",
       " 'Subject16_merged.csv',\n",
       " 'Subject14_merged_fused.csv',\n",
       " 'Subject1_SpO2HR_merged.csv',\n",
       " 'Subject13_SpO2HR.csv',\n",
       " 'Subject14_AccTempEDA.hea',\n",
       " 'Subject9_AccTempEDA.csv',\n",
       " 'Subject8_SpO2HR_sync.csv',\n",
       " 'Subject6_AccTempEDA.hea',\n",
       " 'Subject11_AccTempEDA.csv',\n",
       " 'Subject6_SpO2HR_sync.csv',\n",
       " 'Subject6_AccTempEDA.atr',\n",
       " 'Subject11_merged_fused.csv',\n",
       " 'Subject3_AccTempEDA.csv',\n",
       " 'Subject14_SpO2HR.csv',\n",
       " 'Subject7_SpO2HR_merged.csv',\n",
       " 'Subject1_SpO2HR_sync_merged.csv',\n",
       " 'Subject14_AccTempEDA.atr',\n",
       " 'Subject4_AccTempEDA.hea',\n",
       " 'Subject9_merged.csv',\n",
       " 'Subject15_SpO2HR_sync.csv',\n",
       " 'Subject11_merged.csv',\n",
       " 'Subject19_SpO2HR.hea',\n",
       " 'Subject16_AccTempEDA.hea',\n",
       " 'Subject19_AccTempEDA.csv',\n",
       " 'Subject2_SpO2HR.dat',\n",
       " 'Subject1_SpO2HR.hea',\n",
       " 'Subject1_SpO2HR.csv',\n",
       " 'Subject1_AccTempEDA.atr',\n",
       " 'Subject19_AccTempEDA.hea',\n",
       " 'Subject19_SpO2HR.csv',\n",
       " 'Subject16_AccTempEDA.csv',\n",
       " 'Subject13_AccTempEDA.atr',\n",
       " 'Subject6_SpO2HR_sync_merged.csv',\n",
       " 'Subject4_AccTempEDA.csv',\n",
       " 'Subject2_SpO2HR_merged.csv',\n",
       " 'Subject9_SpO2HR_sync.csv',\n",
       " 'Subject20_SpO2HR.dat',\n",
       " 'Subject3_AccTempEDA.hea',\n",
       " 'Subject14_SpO2HR.hea',\n",
       " 'Subject17_SpO2HR.dat',\n",
       " 'Subject11_AccTempEDA.hea',\n",
       " 'Subject9_AccTempEDA.atr',\n",
       " 'Subject4_merged.csv',\n",
       " 'Subject2_AccTempEDA.hea',\n",
       " 'Subject5_SpO2HR_sync_merged.csv',\n",
       " 'SHA256SUMS.txt',\n",
       " 'Subject4_SpO2HR.csv',\n",
       " 'Subject8_AccTempEDA.atr',\n",
       " 'Subject10_AccTempEDA.hea',\n",
       " 'Subject6_SpO2HR_merged.csv',\n",
       " 'Subject19_merged.csv',\n",
       " 'Subject17_AccTempEDA.csv',\n",
       " 'Subject18_AccTempEDA.hea',\n",
       " 'Subject1_merged.csv',\n",
       " 'Subject4_merged_fused.csv',\n",
       " 'Subject9_SpO2HR.hea',\n",
       " 'Subject5_AccTempEDA.csv',\n",
       " 'Subject20_merged_fused.csv',\n",
       " 'Subject12_SpO2HR.dat',\n",
       " 'Subject11_SpO2HR.hea',\n",
       " 'Subject20_SpO2HR_sync.csv',\n",
       " 'Subject12_AccTempEDA.atr',\n",
       " 'Subject11_SpO2HR.csv',\n",
       " 'Subject9_SpO2HR.csv',\n",
       " 'Subject5_AccTempEDA.hea',\n",
       " 'Subject2_SpO2HR_sync_merged.csv',\n",
       " 'Subject18_AccTempEDA.csv',\n",
       " 'Subject12_SpO2HR_sync.csv',\n",
       " 'Subject8_SpO2HR_sync_merged.csv',\n",
       " 'Subject17_AccTempEDA.hea',\n",
       " 'Subject3_SpO2HR_merged.csv',\n",
       " 'Subject15_merged_fused.csv',\n",
       " 'Subject7_AccTempEDA.atr',\n",
       " 'Subject4_SpO2HR.hea',\n",
       " 'Subject7_SpO2HR.dat',\n",
       " 'Subject10_AccTempEDA.csv',\n",
       " 'Subject15_AccTempEDA.atr',\n",
       " 'Subject1_SpO2HR_sync.csv',\n",
       " 'Subject14_merged.csv',\n",
       " 'Subject2_AccTempEDA.csv']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject2_merged.csv has missing values: False\n",
      "Subject17_merged.csv has missing values: False\n",
      "Subject20_merged.csv has missing values: False\n",
      "Subject12_merged.csv has missing values: False\n",
      "Subject7_merged.csv has missing values: False\n",
      "Subject18_merged.csv has missing values: False\n",
      "Subject15_merged.csv has missing values: False\n",
      "Subject8_merged.csv has missing values: False\n",
      "Subject10_merged.csv has missing values: False\n",
      "Subject5_merged.csv has missing values: False\n",
      "Subject13_merged.csv has missing values: False\n",
      "Subject6_merged.csv has missing values: False\n",
      "Subject3_merged.csv has missing values: False\n",
      "Subject16_merged.csv has missing values: False\n",
      "Subject9_merged.csv has missing values: False\n",
      "Subject11_merged.csv has missing values: False\n",
      "Subject4_merged.csv has missing values: False\n",
      "Subject19_merged.csv has missing values: False\n",
      "Subject1_merged.csv has missing values: False\n",
      "Subject14_merged.csv has missing values: False\n"
     ]
    }
   ],
   "source": [
    "# checking for missing data in each file\n",
    "for file in os.listdir():\n",
    "    for i in range(1,21):\n",
    "        if file.startswith(\"Subject\" + str(i)+ \"_merged.csv\"):\n",
    "            \n",
    "            # reading in file\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            # check data for missing values - should be no missing values throughout\n",
    "            print(f'{file} has missing values: {df.isnull().values.any()}')\n",
    "           \n",
    "            # no missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject2_merged.csv has 1833 outliers\n",
      "Subject17_merged.csv has 1825 outliers\n",
      "Subject20_merged.csv has 2017 outliers\n",
      "Subject12_merged.csv has 1823 outliers\n",
      "Subject7_merged.csv has 1818 outliers\n",
      "Subject18_merged.csv has 1826 outliers\n",
      "Subject15_merged.csv has 1819 outliers\n",
      "Subject8_merged.csv has 1826 outliers\n",
      "Subject10_merged.csv has 1824 outliers\n",
      "Subject5_merged.csv has 1825 outliers\n",
      "Subject13_merged.csv has 1824 outliers\n",
      "Subject6_merged.csv has 1824 outliers\n",
      "Subject3_merged.csv has 1826 outliers\n",
      "Subject16_merged.csv has 1823 outliers\n",
      "Subject9_merged.csv has 1825 outliers\n",
      "Subject11_merged.csv has 2026 outliers\n",
      "Subject4_merged.csv has 1825 outliers\n",
      "Subject19_merged.csv has 1822 outliers\n",
      "Subject1_merged.csv has 1824 outliers\n",
      "Subject14_merged.csv has 1823 outliers\n"
     ]
    }
   ],
   "source": [
    "# referencing https://www.sciencedirect.com/science/article/pii/S2307187724000452 here for background research on time series outlier detection -- also detecting outliers within each subject because data is subject specific \n",
    "\n",
    "# using isolation forest for outlier detection because of strengths listed in paper and common implementation in machine learning\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def detect_outliers_isolation_forest(df, contamination=.05, random_state = 42):\n",
    "    \"\"\"\"\n",
    "    Detects outliers in a dataframe using Isolation Forest\n",
    "    Returns a list of the indices of the outliers\n",
    "    df: the dataframe to detect outliers in\n",
    "    contamination: the proportion of outliers in the data\n",
    "    random_state: the random state to use for reproducibility\n",
    "    \"\"\"\"\"\n",
    "\n",
    "    # selecting numeric features then converting to a list of features to use for outlier detection\n",
    "    features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # removing the physiological stress column because it won't be helpful in detecting outliers in the other features -- this column is binary and will not contribute to the outlier detection process\n",
    "    if 'physiological_stress' in features:\n",
    "        features.remove('physiological_stress')\n",
    "    \n",
    "    # initializing the Isolation Forest model with the specified contamination, random state, and other parameters\n",
    "    clf = IsolationForest(contamination=contamination, random_state=random_state, n_estimators=100, max_samples='auto')\n",
    "\n",
    "    # fitting the model to the data using the selected features\n",
    "    outlier_labels = clf.fit_predict(df[features])\n",
    "\n",
    "    # The fit_predict method returns 1 for inliers and -1 for outliers - creating a mask to identify outliers for the function\n",
    "    outlier_mask = outlier_labels == -1\n",
    "\n",
    "    # returning the outlier mask which is a boolean array indicating which rows are outliers\n",
    "    return outlier_mask\n",
    "\n",
    "# utilizing outlier detection funcâ€ ion\n",
    "for file in os.listdir():\n",
    "    for i in range(1,21):\n",
    "        if file.startswith(\"Subject\" + str(i)+ \"_merged.csv\"):\n",
    "\n",
    "            # reading in the merged dataframe for the subject\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            # checking for outliers using the Isolation Forest method\n",
    "            outlier_mask = detect_outliers_isolation_forest(df, contamination=.10, random_state=42)\n",
    "            \n",
    "            # printing out the number of outliers detected in the dataframe\n",
    "            n_outliers = sum(outlier_mask)\n",
    "            print(f'{file} has {sum(outlier_mask)} outliers')\n",
    "           \n",
    "            # important to note that the outliers are not removed from the data frame and this step is more so focused on detecting outliers\n",
    "\n",
    "            # expecting to have more outliers in this data because of variability in nature of physiological data and the fact that the data is subject specific\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sensor fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor fusion focused on combining data across sensors to provide better accuracy than from just one data source\n",
    "\n",
    "# while the data is all from one sensor in my case - the data from the Empatica wristaband - the data is still from multiple sensors (SpO2HR, Acc, Temp, EDA) and can be combined to provide a more accurate model\n",
    "\n",
    "# sensor fusion is a little bit like feature engineering that I have used in other machine learning projects\n",
    "\n",
    "\n",
    "# initializing a function for sensor fusion -- using a function so I can easily apply it to each subject's merged dataframe and perform the same operations on each one while also demonstrating when there are specific issues in my implementation\n",
    "\n",
    "def sensor_fusion(df):\n",
    "    \"\"\"\n",
    "    Combines data from multiple sensors into a single feature\n",
    "    Returns a dataframe with the sensor fusion features\n",
    "    df: the dataframe to perform sensor fusion on\n",
    "    \"\"\"\n",
    "    fused_df = df.copy()\n",
    "\n",
    "    # creating an acceleration magnitude feature - degree of physical activity\n",
    "    \n",
    "    # checking if there are any acceleration columns in the dataframe, which there should be\n",
    "    actual_acc_columns = []\n",
    "\n",
    "    # checking for acceleration columns in the dataframe - looking for columns that contain 'ax', 'ay', or 'az' in their names given these are my specific acceleration axes and corresponding column names\n",
    "    for col in df.columns:\n",
    "        if any(acc_axis in col.lower() for acc_axis in ['ax', 'ay', 'az']):\n",
    "            actual_acc_columns.append(col)\n",
    "\n",
    "    # checking to see if there are all three axes for acceleration data - if there are not all three axes, we cannot compute the magnitude\n",
    "    if len(actual_acc_columns) >= 3:\n",
    "\n",
    "        # sorting the actual acceleration columns to ensure they are in a consistent order for calculating the magnitude\n",
    "        actual_acc_columns = sorted(actual_acc_columns)\n",
    "\n",
    "        # calculating the magnitude of acceleration using the formula sqrt(ax^2 + ay^2 + az^2) - this is a common way to calculate the magnitude of a 3D vector\n",
    "        fused_df['acc_magnitude'] = np.sqrt(df[actual_acc_columns[0]]**2 + df[actual_acc_columns[1]]**2 + df[actual_acc_columns[2]]**2)\n",
    "\n",
    "    # creating activity adjusted heart rate feature because heart rate generally increases with increased physical activity, as measured by acceleration magnitude\n",
    "\n",
    "    # checking if 'hr' column exists in the dataframe and if 'acc_magnitude' has been created\n",
    "    if 'hr' in df.columns and 'acc_magnitude' in fused_df.columns:\n",
    "\n",
    "        # adjusting heart rate by dividing it by (1 + acceleration magnitude) - this is a simple way to adjust heart rate based on physical activity\n",
    "        fused_df['hr_activity_adjusted'] = fused_df['hr'] / (1 + fused_df['acc_magnitude'])\n",
    "    \n",
    "    # creating temperature to EDA ratio feature because body temperature generally increases with physical activity, and increased body temperature can be manifested physically through sweat which is measured by electro-dermal activity -- finding a ratio of body temperature to EDA can provide a measure of how much someone is sweating relative to their body temperature\n",
    "\n",
    "    if 'temp' in df.columns and 'EDA' in df.columns:\n",
    "\n",
    "        # creating a new feature that is the ratio of temperature to EDA - adding a small constant to EDA to avoid division by zero\n",
    "        fused_df['temp_eda_ratio'] = fused_df['temp'] / (fused_df['EDA'] + .001)\n",
    "\n",
    "    # returning the fused dataframe with the new features\n",
    "    return fused_df\n",
    "\n",
    "# iterating through each subject and applying the sensor fusion function to each subject's merged dataframe, then returning the fused dataframe to a new CSV file\n",
    "for file in os.listdir():\n",
    "    for i in range(1,21):\n",
    "        if file.startswith(\"Subject\" + str(i)+ \"_merged.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "            fused_df = sensor_fusion(df)\n",
    "            fused_df.to_csv(\"Subject\" + str(i) + \"_merged_fused.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# further feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially decided to return to this after running my models to see if it would be necessary \n",
    "\n",
    "# given strength of models on initial features and sensor fused features, I do not think further feature engineering is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will return back to this step to see if it is necessary to use discretization in the final model - discretization is the process of converting continuous data into discrete data, which can be useful for some models\n",
    "\n",
    "# after running the models, I found that the models performed well without discretization - but this is something I can always return to in the future if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all features for now, will consider removing if some features are not useful and detract from in the final model as based on feature importance\n",
    "\n",
    "# Given strength of models with all features, I do not think further feature engineering is necessary at this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary here because all data is numeric -- ie: data isn't image or text based so there aren't features to extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucessfully combined 20 dataframes into one\n"
     ]
    }
   ],
   "source": [
    "# initially was using leave one subject out (LOSO) cross validation out because of individual data frames for each subject -- each subject is a different person and the data is subject specific, but given the computational requirements for LOSO and my computational restrictions for this project, I decided to switch to stratified K-fold cross-validation with instead\n",
    "\n",
    "# stratified K-fold cross-validation is a method that ensures each fold has a representative sample of the data from each subject, which is important for generalizing the model to new subjects\n",
    "\n",
    "\n",
    "# setting up the data for modeling - reading in all the fused dataframes and combining them into a single dataframe for modeling\n",
    "\n",
    "# creating an empty list to store all the dataframes for each subject\n",
    "all_subjects = []\n",
    "\n",
    "# creating empty list to store subject_ids from each data frame\n",
    "subject_ids = []\n",
    "\n",
    "# iterating through each file in the current directory\n",
    "for file in os.listdir():\n",
    "    for i in range(1,21):\n",
    "        if file.startswith(f\"Subject{i}_merged_fused.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            # adding column for subject_id to keep track of which subject the data came from\n",
    "            df['subject_id'] = i\n",
    "\n",
    "            # appending the dataframe to the list of all subjects\n",
    "            all_subjects.append(df)\n",
    "            break\n",
    "\n",
    "# checking if all_subjects list is not empty before concatenating\n",
    "if all_subjects:\n",
    "\n",
    "    # concatenating all the dataframes in the list into a single dataframe\n",
    "    all_subjects_df = pd.concat(all_subjects, ignore_index=True)\n",
    "\n",
    "# checking to see if the code worked correctly\n",
    "    print(f\"sucessfully combined {len(all_subjects)} dataframes into one\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368748, 13)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# investigating the shape of the all_subjects_df to ensure combining went correctly\n",
    "all_subjects_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "physiological_stress\n",
       "0    0.994517\n",
       "1    0.005483\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining balanced class distribution of the physiological stress column\n",
    "all_subjects_df['physiological_stress'].value_counts()/all_subjects_df.shape[0]\n",
    "# where 1 represents physiological stress and 0 represents no physiological stress... very unbalanced\n",
    "# here there are minority class samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, y_train, X_test, y_test, model_name, model, use_smote=True, random_seed=42, use_weights=False):\n",
    "    \"\"\"\n",
    "    Train and evaluate a single model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : Training data\n",
    "    X_test, y_test : Test data\n",
    "    model_name : Name of the model for reporting\n",
    "    model : The model object to train\n",
    "    use_smote : Whether to apply SMOTE\n",
    "    use_weights: Whether to use class weights for imbalanced data\n",
    "    random_seed : Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Performance metrics and trained model\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    X_train_final, y_train_final = X_train_scaled, y_train  # Default to original data\n",
    "\n",
    "    # looking at minority/majority classes to find issues\n",
    "    unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "    minority_class = unique_classes[np.argmin(class_counts)]\n",
    "    majority_class = unique_classes[np.argmax(class_counts)]\n",
    "    imbalance_ratio = np.max(class_counts) / np.min(class_counts)\n",
    "    print(f\"Class imbalance ratio: {(imbalance_ratio):.2f}\")\n",
    "\n",
    "    # Apply SMOTE\n",
    "    if use_smote == True and not use_weights:\n",
    "        try:\n",
    "            # Count samples in each class\n",
    "            unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "            min_samples = np.min(class_counts)\n",
    "            \n",
    "            # Only apply SMOTE if enough samples -- had issues here\n",
    "            if min_samples > 5:\n",
    "                smote = SMOTETomek(random_state=random_seed)\n",
    "                # smote = SMOTE(\n",
    "                #     random_state=random_seed, \n",
    "                #     k_neighbors=min(5, min_samples-1)\n",
    "                # )\n",
    "                X_train_final, y_train_final = smote.fit_resample(X_train_scaled, y_train)\n",
    "                print(f\"  After SMOTE - X_train: {X_train_final.shape}, y_train: {y_train_final.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in SMOTE: {e}. Using original data.\")\n",
    "            X_train_final, y_train_final = X_train_scaled, y_train\n",
    "\n",
    "\n",
    "    elif use_weights == True:\n",
    "        try: \n",
    "            # balanced weights as number of samples/(number of classes * np.bincount(y))\n",
    "            n_samples = len(y_train)\n",
    "            class_weights = {}\n",
    "            n_classes = len(unique_classes)\n",
    "\n",
    "            for cls, count in zip(unique_classes, class_counts):\n",
    "                if cls == minority_class:\n",
    "                    # for minority class, double the weight to counteract imbalance + be more aggressive\n",
    "                    class_weights[cls] = (n_samples / (n_classes * count))*2\n",
    "                else:\n",
    "                    class_weights[cls] = n_samples / (n_classes * count)\n",
    "            \n",
    "            print(\"Using class_weights:\", class_weights)\n",
    "           \n",
    "            if model_name == \"MLP Classifier\":\n",
    "                print( \"  MLP Classifier does not support class weights directly. Using sample weights instead.\")\n",
    "            elif hasattr(model, 'class_weight'):\n",
    "                model.class_weight = class_weights\n",
    "                print(f\"  Using class weights: {class_weights}\")\n",
    "            \n",
    "            elif hasattr(model, 'set_params'):\n",
    "                try: \n",
    "                    model.set_params(class_weight=class_weights)\n",
    "                except Exception: \n",
    "                    print(\"  Model does not support class weights.\")\n",
    "\n",
    "                    # XGboost uses scale_pos_weight parameter\n",
    "                    if model_name == \"XGBoost\" and len(unique_classes) == 2:\n",
    "                        scale_pos_weight = imbalance_ratio \n",
    "                        model.set_params(scale_pos_weight=scale_pos_weight)\n",
    "                        print(f\"  Using scale_pos_weight={scale_pos_weight} for XGBoost.\")\n",
    "                    \n",
    "                    # HGBC uses class_weights = 'balanced' for imbalanced data\n",
    "                    elif model_name == 'Hist Gradient Boosting':\n",
    "                        model.set_params(class_weight='balanced')\n",
    "                        print(\"  Using class_weight='balanced' for Hist Gradient Boosting.\")\n",
    "      \n",
    "       # Handle any exceptions in setting class weights\n",
    "        except Exception as e:\n",
    "            print(f\"  Error calculating class weights: {e}. Using original data.\")\n",
    "\n",
    "\n",
    "    # Train given model on data\n",
    "    try:\n",
    "        print(f\"  Training {model_name}...\")\n",
    "\n",
    "        threshold = .3 # to favor minority class\n",
    "        model.fit(X_train_final, y_train_final)\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_prob = model.predict_proba(X_test_scaled)\n",
    "\n",
    "            # Get index of positive class (1)\n",
    "            pos_class_idx = np.where(model.classes_ == 1)[0][0]  \n",
    "            y_pred_prob = y_prob[:, pos_class_idx]\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            y_pred = (y_pred_prob >= threshold).astype(int)  \n",
    "            \n",
    "            # changing predictions to original class labels\n",
    "\n",
    "            y_pred = np.where(y_pred == 1, minority_class, majority_class)\n",
    "        else:\n",
    "            # Predict and evaluate\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # other metrics but avoiding division by zero errors\n",
    "        try: \n",
    "            recall = recall_score(y_test, y_pred, pos_label = minority_class)\n",
    "        except:\n",
    "            recall = 0.0\n",
    "            print(\"  Warning: Recall calculation failed. Setting recall to 0.0.\")\n",
    "        \n",
    "        try: \n",
    "            precision = precision_score(y_test, y_pred, pos_label=minority_class, zero_division=0)\n",
    "        except:\n",
    "            precision = 0.0\n",
    "            print(\"  Warning: Precision calculation failed. Setting precision to 0.0.\")\n",
    "        \n",
    "        try:\n",
    "            f1 = f1_score(y_test, y_pred, pos_label=minority_class)\n",
    "        except:\n",
    "            f1 = 0.0\n",
    "            print(\"  Warning: F1 score calculation failed. Setting F1 to 0.0.\")\n",
    "\n",
    "        y_test_binary = (y_test == minority_class).astype(int)\n",
    "\n",
    "        # calculating roc and auc if possible\n",
    "\n",
    "        roc_auc = None\n",
    "        pr_auc = None\n",
    "\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(y_test_binary, y_prob[:, pos_class_idx])\n",
    "\n",
    "                pr_auc = average_precision_score(y_test_binary, y_prob)\n",
    "\n",
    "                print(f\" {model_name} ROC AUC: {roc_auc:.4f}, {model_name}PR AUC: {pr_auc:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(\"error calculating AUC metrics:{e}\")\n",
    "        else:\n",
    "            print(f\"  {model_name} does not support ROC AUC or PR AUC calculation.\")\n",
    "        \n",
    "        print(f\"confusion matrix:\\n{cm}\")\n",
    "                \n",
    "        # Return results\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "            'model': model,\n",
    "            'recall': recall\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  Error training {model_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparative_results(all_results, summary):\n",
    "    \"\"\"Plot comparative results of different models including PR-AUC\"\"\"\n",
    "    # Create a figure with four subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Prepare data for all metrics\n",
    "    model_names = []\n",
    "    accuracy_data = []\n",
    "    recall_data = []\n",
    "    roc_auc_data = []\n",
    "    pr_auc_data = []\n",
    "    \n",
    "    for model_name, results in all_results.items():\n",
    "        if results:\n",
    "            # Add to model names\n",
    "            model_names.append(model_name)\n",
    "            \n",
    "            # Extract metrics\n",
    "            accuracies = [r['accuracy'] for r in results]\n",
    "            accuracy_data.append(accuracies)\n",
    "            \n",
    "            recalls = [r.get('recall', np.nan) for r in results]\n",
    "            recall_data.append(recalls)\n",
    "            \n",
    "            # Handle ROC AUC\n",
    "            roc_aucs = []\n",
    "            for r in results:\n",
    "                if 'roc_auc' in r and r['roc_auc'] is not None:\n",
    "                    roc_aucs.append(r['roc_auc'])\n",
    "                else:\n",
    "                    roc_aucs.append(np.nan)\n",
    "            roc_auc_data.append(roc_aucs)\n",
    "            \n",
    "            # Handle PR AUC (Precision-Recall AUC)\n",
    "            pr_aucs = []\n",
    "            for r in results:\n",
    "                if 'pr_auc' in r and r['pr_auc'] is not None:\n",
    "                    pr_aucs.append(r['pr_auc'])\n",
    "                else:\n",
    "                    pr_aucs.append(np.nan)\n",
    "            pr_auc_data.append(pr_aucs)\n",
    "    \n",
    "    # Plot accuracy boxplot\n",
    "    if accuracy_data:\n",
    "        axes[0].boxplot(accuracy_data, labels=model_names)\n",
    "        axes[0].set_ylabel('Balanced Accuracy')\n",
    "        axes[0].set_title('Balanced Accuracy Comparison')\n",
    "        axes[0].grid(True, linestyle='--', alpha=0.7)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot recall boxplot\n",
    "    if recall_data:\n",
    "        axes[1].boxplot(recall_data, labels=model_names)\n",
    "        axes[1].set_ylabel('Recall (Minority Class)')\n",
    "        axes[1].set_title('Recall Comparison')\n",
    "        axes[1].grid(True, linestyle='--', alpha=0.7)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot ROC AUC boxplot (only for models with AUC values)\n",
    "    valid_roc_auc_data = []\n",
    "    valid_roc_model_names = []\n",
    "    \n",
    "    for i, (model_name, auc_values) in enumerate(zip(model_names, roc_auc_data)):\n",
    "        if auc_values and not all(np.isnan(auc_values)):\n",
    "            valid_roc_auc_data.append([val for val in auc_values if not np.isnan(val)])\n",
    "            valid_roc_model_names.append(model_name)\n",
    "    \n",
    "    if valid_roc_auc_data:\n",
    "        axes[2].boxplot(valid_roc_auc_data, labels=valid_roc_model_names)\n",
    "        axes[2].set_ylabel('ROC AUC')\n",
    "        axes[2].set_title('ROC AUC Comparison')\n",
    "        axes[2].grid(True, linestyle='--', alpha=0.7)\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'No ROC AUC data available', \n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "        axes[2].set_title('ROC AUC Comparison')\n",
    "    \n",
    "    # Plot PR AUC boxplot (only for models with PR AUC values)\n",
    "    valid_pr_auc_data = []\n",
    "    valid_pr_model_names = []\n",
    "    \n",
    "    for i, (model_name, auc_values) in enumerate(zip(model_names, pr_auc_data)):\n",
    "        if auc_values and not all(np.isnan(auc_values)):\n",
    "            valid_pr_auc_data.append([val for val in auc_values if not np.isnan(val)])\n",
    "            valid_pr_model_names.append(model_name)\n",
    "    \n",
    "    if valid_pr_auc_data:\n",
    "        axes[3].boxplot(valid_pr_auc_data, labels=valid_pr_model_names)\n",
    "        axes[3].set_ylabel('Precision-Recall AUC')\n",
    "        axes[3].set_title('PR AUC Comparison (Best for Imbalanced Data)')\n",
    "        axes[3].grid(True, linestyle='--', alpha=0.7)\n",
    "        axes[3].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[3].text(0.5, 0.5, 'No PR AUC data available', \n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "        axes[3].set_title('PR AUC Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot confusion matrices for the best model of each type\n",
    "    best_models_cm = {}\n",
    "    \n",
    "    for model_name, results in all_results.items():\n",
    "        if results:\n",
    "            # Find the best model based on PR AUC or recall (better for imbalanced data)\n",
    "            if any('pr_auc' in r and r['pr_auc'] is not None for r in results):\n",
    "                # If PR AUC available, use it\n",
    "                best_idx = np.argmax([r.get('pr_auc', -np.inf) if r.get('pr_auc') is not None else -np.inf \n",
    "                                      for r in results])\n",
    "            elif any('recall' in r for r in results):\n",
    "                # If not, use recall\n",
    "                best_idx = np.argmax([r.get('recall', 0) for r in results])\n",
    "            else:\n",
    "                # Fallback to balanced accuracy\n",
    "                best_idx = np.argmax([r['accuracy'] for r in results])\n",
    "                \n",
    "            best_result = results[best_idx]\n",
    "            \n",
    "            # Store the confusion matrix and metrics\n",
    "            best_models_cm[model_name] = {\n",
    "                'cm': best_result['confusion_matrix'],\n",
    "                'accuracy': best_result['accuracy'],\n",
    "                'recall': best_result.get('recall', None),\n",
    "                'precision': best_result.get('precision', None),\n",
    "                'f1': best_result.get('f1', None),\n",
    "                'roc_auc': best_result.get('roc_auc', None),\n",
    "                'pr_auc': best_result.get('pr_auc', None)\n",
    "            }\n",
    "    \n",
    "    # Plot confusion matrices in a grid\n",
    "    if best_models_cm:\n",
    "        num_models = len(best_models_cm)\n",
    "        cols = min(3, num_models)\n",
    "        rows = (num_models + cols - 1) // cols\n",
    "        \n",
    "        plt.figure(figsize=(5*cols, 4*rows))\n",
    "        \n",
    "        for i, (model_name, data) in enumerate(best_models_cm.items()):\n",
    "            plt.subplot(rows, cols, i+1)\n",
    "            cm = data['cm']\n",
    "            \n",
    "            # Normalize confusion matrix\n",
    "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            cm_norm = np.nan_to_num(cm_norm)  # Replace NaN with 0\n",
    "            \n",
    "            sns.heatmap(cm_norm, annot=cm, fmt='d', cmap='Blues', \n",
    "                        cbar=False, annot_kws={\"size\": 10})\n",
    "            \n",
    "            # Title with multiple metrics\n",
    "            metrics_text = f\"Acc: {data['accuracy']:.4f}\"\n",
    "            \n",
    "            if data['recall'] is not None:\n",
    "                metrics_text += f\"\\nRec: {data['recall']:.4f}\"\n",
    "                \n",
    "            if data['precision'] is not None:\n",
    "                metrics_text += f\", Prec: {data['precision']:.4f}\"\n",
    "                \n",
    "            if data['pr_auc'] is not None:\n",
    "                metrics_text += f\"\\nPR-AUC: {data['pr_auc']:.4f}\"\n",
    "            \n",
    "            plt.title(f\"{model_name}\\n{metrics_text}\")\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Create a summary table of metrics\n",
    "    if model_names:\n",
    "        plt.figure(figsize=(12, len(model_names) * 0.7 + 1))\n",
    "        table_data = []\n",
    "        table_columns = ['Model', 'Accuracy (mean Â± std)', 'Recall (mean Â± std)', \n",
    "                         'Precision (mean Â± std)', 'ROC AUC (mean Â± std)', 'PR AUC (mean Â± std)']\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            if model_name in summary:\n",
    "                model_summary = summary[model_name]\n",
    "                \n",
    "                # Calculate mean/std for each metric\n",
    "                acc_mean = model_summary['mean_accuracy']\n",
    "                acc_std = model_summary['std_accuracy']\n",
    "                \n",
    "                # Extract additional metrics from all_results\n",
    "                recalls = [r.get('recall', np.nan) for r in all_results[model_name] if 'recall' in r]\n",
    "                precisions = [r.get('precision', np.nan) for r in all_results[model_name] if 'precision' in r]\n",
    "                roc_aucs = [r.get('roc_auc', np.nan) for r in all_results[model_name] \n",
    "                          if 'roc_auc' in r and r['roc_auc'] is not None]\n",
    "                pr_aucs = [r.get('pr_auc', np.nan) for r in all_results[model_name] \n",
    "                         if 'pr_auc' in r and r['pr_auc'] is not None]\n",
    "                \n",
    "                # Calculate means and stds if data exists\n",
    "                recall_mean = np.nanmean(recalls) if recalls and not all(np.isnan(recalls)) else np.nan\n",
    "                recall_std = np.nanstd(recalls) if recalls and not all(np.isnan(recalls)) else np.nan\n",
    "                \n",
    "                precision_mean = np.nanmean(precisions) if precisions and not all(np.isnan(precisions)) else np.nan\n",
    "                precision_std = np.nanstd(precisions) if precisions and not all(np.isnan(precisions)) else np.nan\n",
    "                \n",
    "                roc_auc_mean = np.nanmean(roc_aucs) if roc_aucs and not all(np.isnan(roc_aucs)) else np.nan\n",
    "                roc_auc_std = np.nanstd(roc_aucs) if roc_aucs and not all(np.isnan(roc_aucs)) else np.nan\n",
    "                \n",
    "                pr_auc_mean = np.nanmean(pr_aucs) if pr_aucs and not all(np.isnan(pr_aucs)) else np.nan\n",
    "                pr_auc_std = np.nanstd(pr_aucs) if pr_aucs and not all(np.isnan(pr_aucs)) else np.nan\n",
    "                \n",
    "                # Format for table\n",
    "                acc_text = f\"{acc_mean:.4f} Â± {acc_std:.4f}\"\n",
    "                recall_text = f\"{recall_mean:.4f} Â± {recall_std:.4f}\" if not np.isnan(recall_mean) else \"N/A\"\n",
    "                precision_text = f\"{precision_mean:.4f} Â± {precision_std:.4f}\" if not np.isnan(precision_mean) else \"N/A\"\n",
    "                roc_auc_text = f\"{roc_auc_mean:.4f} Â± {roc_auc_std:.4f}\" if not np.isnan(roc_auc_mean) else \"N/A\"\n",
    "                pr_auc_text = f\"{pr_auc_mean:.4f} Â± {pr_auc_std:.4f}\" if not np.isnan(pr_auc_mean) else \"N/A\"\n",
    "                \n",
    "                table_data.append([\n",
    "                    model_name, acc_text, recall_text, precision_text, roc_auc_text, pr_auc_text\n",
    "                ])\n",
    "        \n",
    "        # Create and plot the table\n",
    "        table = plt.table(cellText=table_data,\n",
    "                          colLabels=table_columns,\n",
    "                          loc='center',\n",
    "                          cellLoc='center',\n",
    "                          colWidths=[0.15, 0.17, 0.17, 0.17, 0.17, 0.17])\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 1.5)\n",
    "        \n",
    "        # Hide axes\n",
    "        plt.axis('off')\n",
    "        plt.title('Summary of Model Performance', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# adapted from GWU DATS 6202 Machine Learning\n",
    "def run_comparative_analysis(X, y, random_seed=42, use_smote=True, use_weights = False, tune_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Run comparative analysis of multiple models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : Features\n",
    "    y : Target\n",
    "    random_seed : Random seed for reproducibility\n",
    "    use_smote : Whether to apply SMOTE function\n",
    "    tune_hyperparams : Whether to tune hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results for all models and folds\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "        X_data = X.values\n",
    "        feature_names = X.columns\n",
    "    else:\n",
    "        X_data = np.array(X)\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(X_data.shape[1])]\n",
    "        \n",
    "    if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "        y_data = y.values\n",
    "    else:\n",
    "        y_data = np.array(y)\n",
    "    \n",
    "    # Ensure same number of samples\n",
    "    if len(X_data) != len(y_data):\n",
    "        min_len = min(len(X_data), len(y_data))\n",
    "        X_data = X_data[:min_len]\n",
    "        y_data = y_data[:min_len]\n",
    "        print(f\"Trimmed arrays to match: {min_len} samples\")\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'XGBoost': XGBClassifier(\n",
    "            use_label_encoder=False, # suppress warning about label encoder\n",
    "            eval_metric='logloss', # using logloss for binary classification\n",
    "            random_state=random_seed, # for continuity \n",
    "            verbosity=0 # don't print out too much info during training\n",
    "        ),\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000, # maximum number of iterations for convergence\n",
    "            random_state=random_seed # for continuity\n",
    "        ),\n",
    "        'MLP Classifier': MLPClassifier(\n",
    "            max_iter=1000, # maximum number of iterations for convergence\n",
    "            random_state=random_seed  # for continuity\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            random_state=random_seed # for continuity\n",
    "        ),\n",
    "        'Hist Gradient Boosting': HistGradientBoostingClassifier(\n",
    "            random_state=random_seed # for continuity\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Define parameter grids for tuning\n",
    "    param_grids = {\n",
    "        'XGBoost': {\n",
    "            'max_depth': [3, 5, 7], # maximum depth of each tree in forest\n",
    "            'n_estimators': [100, 300, 500], # number of boosting rounds\n",
    "            'learning_rate': [0.01, 0.1, 0.3], # learning rate for boosting\n",
    "            'subsample': [0.8, 1.0], # fraction of samples to use for each tree, within training\n",
    "            'colsample_bytree': [0.8, 1.0], # fraction of features to use for each tree\n",
    "            'min_child_weight': [1, 3, 5] # minimum sum of sample weight (hessian) needed to create a new node (child) in a tree\n",
    "        },\n",
    "        'Logistic Regression': {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100], # Inverse of regularization strength; smaller values specify stronger regularization\n",
    "            'penalty': ['l2', None], # Regularization type; 'l2' is standard, None means no regularization\n",
    "            'tol': [1e-5, 1e-4, 1e-3] # Tolerance for stopping criteria; smaller values mean more precise convergence\n",
    "        },\n",
    "        'MLP Classifier': {\n",
    "            'alpha': [0.0001, 0.001, 0.01], # L2 penalty (regularization term) parameter to prevent overfitting by constraiing weights\n",
    "            'learning_rate_init': [0.001, 0.01],  # Initial learning rate used in the optimizer, learning rate determines how much parameters are adjusted\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50)] # Number of neurons in the hidden layers - usually between input and output size\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [100, 200, 300], # Number of trees in the forest\n",
    "            'max_depth': [None, 10, 20, 30], # Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contain less than min_samples_split samples\n",
    "            'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "            'min_samples_leaf': [1, 2, 4] # Minimum number of samples required to be at a leaf node\n",
    "        },\n",
    "        'Hist Gradient Boosting': {\n",
    "            'max_depth': [None, 3, 5], # Maximum depth of the individual trees\n",
    "            'learning_rate': [0.01, 0.1, 0.3], # Learning rate shrinks the contribution of each tree in HGBC\n",
    "            'max_iter': [100, 200] # Number of boosting iterations (trees to fit)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if use_weights:\n",
    "        # Update models to use class weights if specified\n",
    "        param_grids['Logistic Regression']['class_weight'] = ['balanced', None]\n",
    "        param_grids['Random Forest']['class_weight'] = ['balanced', 'balanced_subsample', None]\n",
    "        param_grids['Hist Gradient Boosting']['class_weight'] = ['balanced', None]\n",
    "\n",
    "        # for XGBoost, we can set scale_pos_weight for imbalanced data\n",
    "        unique_classes = np.unique(y_data)\n",
    "        if len(unique_classes) == 2:  # Binary classification\n",
    "            class_counts = np.bincount(y_data.astype(int))\n",
    "            if len(class_counts) == 2:\n",
    "                ratio = class_counts[0] / class_counts[1]\n",
    "                param_grids['XGBoost']['scale_pos_weight'] = [1, ratio, ratio*.75, ratio*1.25]\n",
    "    \n",
    "    # Create stratified splits with 5 splits for best final model performance\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "    all_results = {model_name: [] for model_name in models.keys()}\n",
    "    \n",
    "    # For storing the best model of each type\n",
    "    best_models = {}\n",
    "    \n",
    "    # Check overall class distribution\n",
    "    unique_classes, class_counts = np.unique(y_data, return_counts=True)\n",
    "    print(\"overall class distribution:\")\n",
    "    for cls, count in zip(unique_classes, class_counts):\n",
    "        print(f\"  Class {cls}: {count} samples ({count / len(y_data) * 100:.2f}%)\")\n",
    "\n",
    "    # Process each fold\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_data, y_data)):\n",
    "        print(f\"\\nProcessing Fold {fold+1}/5\")\n",
    "        \n",
    "        # Get train/test data\n",
    "        X_train, X_test = X_data[train_idx], X_data[test_idx]\n",
    "        y_train, y_test = y_data[train_idx], y_data[test_idx]\n",
    "        \n",
    "        print(f\"Train shapes: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "        print(f\"Test shapes: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "        \n",
    "        train_classes, train_counts = np.unique(y_train, return_counts=True)\n",
    "        print(\"Train class distribution:\")\n",
    "        for cls, count in zip(train_classes, train_counts):\n",
    "            print(f\"  Class {cls}: {count} samples ({count / len(y_train) * 100:.2f}%)\")\n",
    "\n",
    "        # Process each model\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "            \n",
    "            # Apply hyperparameter tuning\n",
    "            if tune_hyperparams:\n",
    "                print(f\"  Tuning hyperparameters for {model_name}...\")\n",
    "                \n",
    "                # Create inner CV for tuning with 3 splits\n",
    "                inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_seed)\n",
    "                \n",
    "                # Scale features for tuning\n",
    "                scaler = MinMaxScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                \n",
    "                # Apply SMOTE for tuning if enabled\n",
    "                if use_smote and not use_weights:\n",
    "                    try:\n",
    "                        unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "                        min_samples = np.min(class_counts)\n",
    "                        \n",
    "                        if min_samples > 5:\n",
    "                            smote = SMOTETomek(random_state=random_seed)\n",
    "                            # smote = SMOTE(\n",
    "                            #     random_state=random_seed,\n",
    "                            #     k_neighbors=min(5, min_samples-1)\n",
    "                            # )\n",
    "                            X_train_scaled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "                        else:\n",
    "                            y_train_resampled = y_train\n",
    "\n",
    "                    # raise error if issue using SMOTE\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error in SMOTE for tuning: {e}. Using original data.\")\n",
    "                        y_train_resampled = y_train\n",
    "                else:\n",
    "                    y_train_resampled = y_train\n",
    "                \n",
    "                # Create and run randomized search\n",
    "                try:\n",
    "                    search = RandomizedSearchCV(\n",
    "                        model, # the model to tune\n",
    "                        param_distributions=param_grids[model_name], # using the parameter grid defined above\n",
    "                        n_iter=10, # number of parameter settings to sample\n",
    "                        scoring='balanced_accuracy', # using balanced accuracy for evaluation\n",
    "                        cv=inner_cv, # inner cross-validation for tuning\n",
    "                        random_state=random_seed, # for continuity\n",
    "                        n_jobs=-1, # use all available cores\n",
    "                        verbose=0 # less output during tuning\n",
    "                    )\n",
    "                    search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "                    # pull best parameters and best estimator for the given model\n",
    "                    best_params = search.best_params_\n",
    "                    tuned_model = search.best_estimator_\n",
    "                    \n",
    "                    print(f\"  Best parameters: {best_params}\")\n",
    "                    \n",
    "                    # Evaluate the tuned model using evaluate_model function which also produces cm and classification report\n",
    "                    result = evaluate_model(\n",
    "                        X_train, y_train, X_test, y_test,\n",
    "                        model_name, tuned_model,\n",
    "                        use_smote=(use_smote and not use_weights),\n",
    "                        use_weights = use_weights,\n",
    "                        random_seed=random_seed\n",
    "                    )\n",
    "                    \n",
    "                    if result:\n",
    "                        result['fold'] = fold + 1\n",
    "                        result['best_params'] = best_params\n",
    "                        all_results[model_name].append(result)\n",
    "\n",
    "                # trying to handle errors here \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in hyperparameter tuning for {model_name}: {e}\")\n",
    "                    \n",
    "                    # Fall back to default model without tuned parameters \n",
    "                    result = evaluate_model(\n",
    "                        X_train, y_train, X_test, y_test,\n",
    "                        model_name, model,\n",
    "                        use_smote=use_smote,\n",
    "                        random_seed=random_seed\n",
    "                    )\n",
    "                    \n",
    "                    if result:\n",
    "                        result['fold'] = fold + 1\n",
    "                        result['best_params'] = \"Tuning failed, using default parameters\"\n",
    "                        all_results[model_name].append(result)\n",
    "    \n",
    "    # Calculate average performance for each model\n",
    "    print(\"\\n--- Summary of Results ---\")\n",
    "    summary = {}\n",
    "    \n",
    "    for model_name, results in all_results.items():\n",
    "        if results:\n",
    "            accuracies = [r['accuracy'] for r in results]\n",
    "            mean_accuracy = np.mean(accuracies) # for each model's accuracy across folds\n",
    "            std_accuracy = np.std(accuracies) # standard deviation of accuracies across folds\n",
    "            \n",
    "            print(f\"{model_name}: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
    "            \n",
    "            # Save best model of each type\n",
    "            best_idx = np.argmax(accuracies)\n",
    "            best_models[model_name] = results[best_idx]['model']\n",
    "            \n",
    "            # Save summary stats\n",
    "            summary[model_name] = {\n",
    "                'mean_accuracy': mean_accuracy,\n",
    "                'std_accuracy': std_accuracy,\n",
    "                'individual_accuracies': accuracies\n",
    "            }\n",
    "        else:\n",
    "            # error handling\n",
    "            print(f\"{model_name}: Failed to train\")\n",
    "    \n",
    "    # # Plot comparative results if at least one successful model\n",
    "    # if any(all_results.values()):\n",
    "    #     plot_comparative_results(all_results, summary)\n",
    "    \n",
    "    # return {\n",
    "    #     'all_results': all_results,\n",
    "    #     'summary': summary,\n",
    "    #     'best_models': best_models,\n",
    "    #     'feature_names': feature_names\n",
    "    # }\n",
    "\n",
    "\n",
    "def save_best_model(results):\n",
    "    \"\"\"\n",
    "    Save the best model from all models tested\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : Results from run_comparative_analysis\n",
    "    \"\"\"    \n",
    "    summary = results['summary']\n",
    "    best_models = results['best_models']\n",
    "    \n",
    "    # Find the best model overall\n",
    "    best_model_name = max(summary, key=lambda x: summary[x]['mean_accuracy'])\n",
    "    best_model = best_models[best_model_name]\n",
    "    \n",
    "    # Save best model\n",
    "    \n",
    "    print(f\"Best model ({best_model_name})\")\n",
    "    print(f\"Mean accuracy: {summary[best_model_name]['mean_accuracy']:.4f}\")\n",
    "    \n",
    "    return best_model_name, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importance(results, top_n=10):\n",
    "#     \"\"\"\n",
    "#     Plot feature importance for models that support it\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     results : Results from run_comparative_analysis\n",
    "#     top_n : Number of top features to show\n",
    "#     \"\"\"\n",
    "#     feature_names = results['feature_names']\n",
    "#     best_models = results['best_models']\n",
    "    \n",
    "#     importance_data = {}\n",
    "    \n",
    "#     # Extract feature importance from models that support it -- XGBoost, Random Forest, Hist Gradient Boosting, and Logistic Regression\n",
    "#     for model_name, model in best_models.items():\n",
    "#         if model_name == 'XGBoost':\n",
    "#             importance = model.feature_importances_\n",
    "#             importance_data[model_name] = {\n",
    "#                 'importance': importance, # get feature importances from the model\n",
    "#                 'type': 'direct' # direct importance from the model -- generalized for cross model comparison\n",
    "#             }\n",
    "#         elif model_name == 'Random Forest':\n",
    "#             importance = model.feature_importances_\n",
    "#             importance_data[model_name] = {\n",
    "#                 'importance': importance, \n",
    "#                 'type': 'direct'\n",
    "#             }\n",
    "#         elif model_name == 'Hist Gradient Boosting':\n",
    "#             if hasattr(model, 'feature_importances_'):\n",
    "#                 importance = model.feature_importances_\n",
    "#                 importance_data[model_name] = {\n",
    "#                     'importance': importance,\n",
    "#                     'type': 'direct'\n",
    "#                 }\n",
    "#         elif model_name == 'Logistic Regression':\n",
    "#             if hasattr(model, 'coef_'):\n",
    "#                 importance = np.abs(model.coef_)[0]\n",
    "#                 importance_data[model_name] = {\n",
    "#                     'importance': importance,\n",
    "#                     'type': 'coefficient' # using absolute value of coefficients as importance instead of direct because not applicable w LR\n",
    "#                 }\n",
    "    \n",
    "#     # Plot feature importance for each model\n",
    "#     for model_name, data in importance_data.items():\n",
    "#         importance = data['importance']\n",
    "        \n",
    "#         # Sort features by importance\n",
    "#         indices = np.argsort(importance)[::-1]\n",
    "        \n",
    "#         # Plot top_n features\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "        \n",
    "#         # Limit to top_n features or all features if less than top_n\n",
    "#         n_features = min(top_n, len(feature_names))\n",
    "        \n",
    "#         plt.barh(range(n_features), importance[indices[:n_features]])\n",
    "#         plt.yticks(range(n_features), [feature_names[i] for i in indices[:n_features]])\n",
    "#         plt.xlabel('Feature Importance')\n",
    "#         plt.title(f'Top {n_features} Features for {model_name}')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "    \n",
    "#     return importance_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall class distribution:\n",
      "  Class 0: 366726 samples (99.45%)\n",
      "  Class 1: 2022 samples (0.55%)\n",
      "\n",
      "Processing Fold 1/5\n",
      "Train shapes: X_train (294998, 10), y_train (294998,)\n",
      "Test shapes: X_test (73750, 10), y_test (73750,)\n",
      "Train class distribution:\n",
      "  Class 0: 293380 samples (99.45%)\n",
      "  Class 1: 1618 samples (0.55%)\n",
      "\n",
      "Training XGBoost...\n",
      "  Tuning hyperparameters for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'subsample': 1.0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 0.8}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training XGBoost...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73322    24]\n",
      " [    6   398]]\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Tuning hyperparameters for Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'tol': 1e-05, 'penalty': None, 'C': 1}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training Logistic Regression...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[52762 20584]\n",
      " [    5   399]]\n",
      "\n",
      "Training MLP Classifier...\n",
      "  Tuning hyperparameters for MLP Classifier...\n",
      "  Best parameters: {'learning_rate_init': 0.001, 'hidden_layer_sizes': (100,), 'alpha': 0.0001}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training MLP Classifier...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73134   212]\n",
      " [    0   404]]\n",
      "\n",
      "Training Random Forest...\n",
      "  Tuning hyperparameters for Random Forest...\n",
      "  Best parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training Random Forest...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73325    21]\n",
      " [    5   399]]\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "  Tuning hyperparameters for Hist Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'max_iter': 100, 'max_depth': 5, 'learning_rate': 0.3}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training Hist Gradient Boosting...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73315    31]\n",
      " [    6   398]]\n",
      "\n",
      "Processing Fold 2/5\n",
      "Train shapes: X_train (294998, 10), y_train (294998,)\n",
      "Test shapes: X_test (73750, 10), y_test (73750,)\n",
      "Train class distribution:\n",
      "  Class 0: 293381 samples (99.45%)\n",
      "  Class 1: 1617 samples (0.55%)\n",
      "\n",
      "Training XGBoost...\n",
      "  Tuning hyperparameters for XGBoost...\n",
      "  Best parameters: {'subsample': 1.0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 0.8}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586756, 10), y_train: (586756,)\n",
      "  Training XGBoost...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73329    16]\n",
      " [    6   399]]\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Tuning hyperparameters for Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'tol': 1e-05, 'penalty': None, 'C': 1}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586756, 10), y_train: (586756,)\n",
      "  Training Logistic Regression...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[52910 20435]\n",
      " [    6   399]]\n",
      "\n",
      "Training MLP Classifier...\n",
      "  Tuning hyperparameters for MLP Classifier...\n",
      "  Best parameters: {'learning_rate_init': 0.001, 'hidden_layer_sizes': (100,), 'alpha': 0.0001}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586756, 10), y_train: (586756,)\n",
      "  Training MLP Classifier...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73097   248]\n",
      " [    0   405]]\n",
      "\n",
      "Training Random Forest...\n",
      "  Tuning hyperparameters for Random Forest...\n",
      "  Best parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586756, 10), y_train: (586756,)\n",
      "  Training Random Forest...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73327    18]\n",
      " [    7   398]]\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "  Tuning hyperparameters for Hist Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'max_iter': 200, 'max_depth': 3, 'learning_rate': 0.3}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586756, 10), y_train: (586756,)\n",
      "  Training Hist Gradient Boosting...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73321    24]\n",
      " [    4   401]]\n",
      "\n",
      "Processing Fold 3/5\n",
      "Train shapes: X_train (294998, 10), y_train (294998,)\n",
      "Test shapes: X_test (73750, 10), y_test (73750,)\n",
      "Train class distribution:\n",
      "  Class 0: 293381 samples (99.45%)\n",
      "  Class 1: 1617 samples (0.55%)\n",
      "\n",
      "Training XGBoost...\n",
      "  Tuning hyperparameters for XGBoost...\n",
      "  Best parameters: {'subsample': 1.0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 0.8}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training XGBoost...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73324    21]\n",
      " [    7   398]]\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Tuning hyperparameters for Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'tol': 1e-05, 'penalty': None, 'C': 1}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training Logistic Regression...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[53287 20058]\n",
      " [    8   397]]\n",
      "\n",
      "Training MLP Classifier...\n",
      "  Tuning hyperparameters for MLP Classifier...\n",
      "  Best parameters: {'learning_rate_init': 0.01, 'hidden_layer_sizes': (50,), 'alpha': 0.0001}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training MLP Classifier...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73112   233]\n",
      " [    1   404]]\n",
      "\n",
      "Training Random Forest...\n",
      "  Tuning hyperparameters for Random Forest...\n",
      "  Best parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training Random Forest...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73326    19]\n",
      " [    6   399]]\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "  Tuning hyperparameters for Hist Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'max_iter': 200, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "Class imbalance ratio: 181.44\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training Hist Gradient Boosting...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73318    27]\n",
      " [    6   399]]\n",
      "\n",
      "Processing Fold 4/5\n",
      "Train shapes: X_train (294999, 10), y_train (294999,)\n",
      "Test shapes: X_test (73749, 10), y_test (73749,)\n",
      "Train class distribution:\n",
      "  Class 0: 293381 samples (99.45%)\n",
      "  Class 1: 1618 samples (0.55%)\n",
      "\n",
      "Training XGBoost...\n",
      "  Tuning hyperparameters for XGBoost...\n",
      "  Best parameters: {'subsample': 1.0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 0.8}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training XGBoost...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73325    20]\n",
      " [    3   401]]\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Tuning hyperparameters for Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'tol': 1e-05, 'penalty': None, 'C': 1}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training Logistic Regression...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[53578 19767]\n",
      " [    9   395]]\n",
      "\n",
      "Training MLP Classifier...\n",
      "  Tuning hyperparameters for MLP Classifier...\n",
      "  Best parameters: {'learning_rate_init': 0.01, 'hidden_layer_sizes': (50, 50), 'alpha': 0.0001}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training MLP Classifier...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73070   275]\n",
      " [    0   404]]\n",
      "\n",
      "Training Random Forest...\n",
      "  Tuning hyperparameters for Random Forest...\n",
      "  Best parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training Random Forest...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73324    21]\n",
      " [    1   403]]\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "  Tuning hyperparameters for Hist Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'max_iter': 200, 'max_depth': 3, 'learning_rate': 0.3}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586758, 10), y_train: (586758,)\n",
      "  Training Hist Gradient Boosting...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73319    26]\n",
      " [    0   404]]\n",
      "\n",
      "Processing Fold 5/5\n",
      "Train shapes: X_train (294999, 10), y_train (294999,)\n",
      "Test shapes: X_test (73749, 10), y_test (73749,)\n",
      "Train class distribution:\n",
      "  Class 0: 293381 samples (99.45%)\n",
      "  Class 1: 1618 samples (0.55%)\n",
      "\n",
      "Training XGBoost...\n",
      "  Tuning hyperparameters for XGBoost...\n",
      "  Best parameters: {'subsample': 1.0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 0.8}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training XGBoost...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73329    16]\n",
      " [    4   400]]\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Tuning hyperparameters for Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'tol': 1e-05, 'penalty': None, 'C': 1}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training Logistic Regression...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[52371 20974]\n",
      " [    3   401]]\n",
      "\n",
      "Training MLP Classifier...\n",
      "  Tuning hyperparameters for MLP Classifier...\n",
      "  Best parameters: {'learning_rate_init': 0.01, 'hidden_layer_sizes': (50,), 'alpha': 0.0001}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training MLP Classifier...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73121   224]\n",
      " [    0   404]]\n",
      "\n",
      "Training Random Forest...\n",
      "  Tuning hyperparameters for Random Forest...\n",
      "  Best parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training Random Forest...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73330    15]\n",
      " [    6   398]]\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "  Tuning hyperparameters for Hist Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/evaschwartz/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best parameters: {'max_iter': 100, 'max_depth': 5, 'learning_rate': 0.3}\n",
      "Class imbalance ratio: 181.32\n",
      "  After SMOTE - X_train: (586762, 10), y_train: (586762,)\n",
      "  Training Hist Gradient Boosting...\n",
      "error calculating AUC metrics:{e}\n",
      "confusion matrix:\n",
      "[[73328    17]\n",
      " [    3   401]]\n",
      "\n",
      "--- Summary of Results ---\n",
      "XGBoost: 0.9934 Â± 0.0018\n",
      "Logistic Regression: 0.8535 Â± 0.0003\n",
      "MLP Classifier: 0.9981 Â± 0.0005\n",
      "Random Forest: 0.9937 Â± 0.0026\n",
      "Hist Gradient Boosting: 0.9951 Â± 0.0028\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    X = all_subjects_df.drop(columns=['physiological_stress', 'time', 'subject_id'])\n",
    "    y = all_subjects_df['physiological_stress']\n",
    "    \n",
    "    # Run comparative analysis code\n",
    "    results = run_comparative_analysis(X, y, random_seed=42, use_smote=True, use_weights=False, tune_hyperparams=True)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    # importance_data = plot_feature_importance(results, top_n=10)\n",
    "    \n",
    "    # Save best model\n",
    "    # best_model_name, best_model = save_best_model(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance ratio: 181.37\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "ratio = max(counts) / min(counts)\n",
    "print(f\"Imbalance ratio: {ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models arent predicting positive class at all because imbalance ratio is so extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
